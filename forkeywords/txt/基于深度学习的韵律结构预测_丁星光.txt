


韵律，也称为节律、上加成素、非线性特征
等。虽然语言学家使用的名称不同，分析的方法
也不同，但实际指的是同一概念[1]。韵律结构分
析是语音合成系统的重要组成部分。准确的预测
文本的韵律边界位置以及其等级，是语音合成中
的重要环节，它是合成自然、流畅的输出语音的
重要前提和保证。 

一般认为韵律结构有三个层级，从小到大依
次是韵律词、韵律短语和语调短语。以“致以诚
挚的问候和良好的祝愿”为例，结构如下所示。
其中 PW、PP、IP、S 分别表示韵律词、韵律短
语、语调短语和句子。 

 

图 1  “致以诚挚的问候和良好的祝愿”的韵律结构 

早期的韵律预测方法大多是基于规则的方
法，其思路是从语言学、句法分析入手总结出经
验知识，并将其整理成规则，以映射韵律层级的
生成。基于规则的方法的优点是简单，而且得到
的模型较为直观。而仅仅基于规则的方法也有很
多局限性，随着统计机器学习的发展，越来越多
的学者开始转向基于统计方法的韵律预测。 

习模型将韵律边界预测转化为一个分类问题，根
据词法等信息作为特征。针对韵律短语的预测，
条件随机场、最大熵模型通常都有 70%以上的
f-score 分值[2]。 

不同的韵律预测的研究针对的语言、使用的
语料和对节奏层级的定义都有所不同，对结果采
取的评价方法也不一致，故不能直接比较各模型
方法的优劣。只有在相同训练集和测试集的前提
下，并采取一致的评价标准，才能较为客观的比
较各种方法的优劣。 

本文按照 3:1:1 的比例将语料分为训练集、
交叉验证、测试集三个部分。采用 F-score 来评价
模型的准确性。 

1 语料库以及特征选取 

1.1 语料库的设计 

本文使用的语料库 1 包括 20000 句，总计
400000 多个音节，均进行了三个层级的韵律边界
标注。 

本文使用的语料库 2 有 28000 句，其中每一
句仅有一级短语边界的划分，这个停顿是介于语
调短语和韵律短语之间的。共包括 500000 多个音
节。该语料库主要用于对特征重要性的排序研究。 
本文使用的语料库 3 有 1000 句，每一句有 8
个人进行了韵律短语、语调短语边界的划分。这
部分语料用于检验本文的多样性结论。 
1.2 特征选择 

一些统计机器学习方法在韵律预测中得到了

特征的选取对韵律结构的预测非常重要。本文

广泛的应用，比如决策树模型、隐马尔科夫模型、
最大熵模型、条件随机场模型等等。这些机器学

基于文本特征来预测韵律词、韵律短语和语调短
语。特征包括： 

 

1）NSYL_i, i=sentence, initial, final：整个句子的音节数，当

前词到句首的音节数，当前词到句末的音节数； 

2）Nword_sentence：整个句子的语法词数； 
3）word2veci, i= -2, -1, 0, 1, 2, 3：当前词、前二词、前一

词、后一词、后二词、后三词的词向量； 

4）POSi, i= -2, -1, 0, 1, 2, 3：当前词、前二词、前一词、

后一词、后二词、后三词的词性； 

5）li, i= -2, -1, 0, 1, 2, 3：当前词、前二词、前一词、后

一词、后二词、后三词的词长。 

同时还考虑了句法信息[3][4]，句法分析指的
是对句子中的词语语法功能进行分析。其主要的
应用通常在中文信息处理中。它是语块分析思想
的一个直接体现。 

以“为了使学生回乡后有一定的专业知识”

为例，其结构如下所示： 

图 2“为了使学生回乡后有一定的专业知识”的句法结构 

 

其中句法树相关特征如下所示： 

表 1    句法树相关特征 

当前词和下一词在树中距离  前面第二词在树中层级 

术，将当前样本集分为两个子样本集，使得生成
的每个叶子节点都有两个分支。其基本思想为将
训练样本进行递归地划分自变量空间进行建树，
并用验证数据进行剪枝。 

为了更好的拟合残差，本文还使用梯度渐进
回归树方法(Gradient Boosting Regression Tree) ，
GBRT 比较重要的超参数有最大叶子节点数、学
习速率、森林个数以及正负样本比例。 

梯度渐进回归树方法的思想是每一颗树学

习的是之前所有树结果的和的残差。 

这里的 Boost 指的并不是重采样的迭代，也
不是 Adaboost，而是对目标样本的迭代。和传统
的 Adaboost 的区别在于每一次的计算是为了减
少上一次计算的残差，即在残差减少的梯度上建
立一个新的模型。这里每一次迭代的时候，样本
集均保持不变，更新的是每一个样本的目标值。 

表 2    GBRT 决策树模型的 F-score 结果 

学习速率  树的个数 

  语料库 1-  

语料库 1-    

 韵律短语 

语调短语 

0.001 

0.0001 

0.0001 

0.0007 

---- 

10 

10 

100 

23 

--- 

          70.94% 

70.13% 

69.13% 

59.31% 

73.01% 

71.72% 

67.77% 

58.24% 

66.95% 

65.13% 

注：最后一行为 CART 的对比结果 
2.2 条件随机场模型 

对于条件随机场（CRF）模型，需要进行组
合特征的选取。本文组合特征选取的思路是采用
类似贪心法的原则，即以此加入可能的二元组合
特征，如果在交叉验证的基础上有一定 f-score 效
果的提升，则认为此特征是较好的。 

CRF 模型比较重要的超参数包括过拟合参

当前词和前一词在树中距离

前一词在树中层级 

数 float 等。 

当前词在树前序遍历位置 

当前词在树中层级 

表 4    条件随机场模型的 F-score 结果 

当前词在树中序遍历位置 

后一词在树中层级 

当前词在树后序遍历位置 

后面第二词在树中层级 

当前词的父节点包含音节数 

当前词的子节点包含音节数 

后面第三词在树中层级 
整个句子句法树深度   

2 单模型分类器的设计及实验结果 

在分类器设计方面，本文使用了决策树模
型、神经网络模型以及条件随机场模型来进行韵
律结构预测。交叉验证的时候采用 3:1 的比例。 
2.1 决策树模型 

分类回归树算法，即 CART(Classification And 
Regression  Tree)算法采用一种二分递归分割的技

过拟合参数 

特征个数 

2 

1 

0.6 

0.6 

0.1 

149 

149 

406 

149 

149 

语料库 1- 

韵律短语 

   62.8% 

74.3% 

66.9% 

77.7% 

65.2% 

2.3 深度学习模型 

对于神经网络模型，采用深度学习中最近比
较常用的递归神经网络（Recurrent  Nueral  Net-

 

work,RNN）。与较早的人工神经网络（ANN）不
同，RNN 隐含层里包含的传播信息更加丰富[5] [6]。 
RNN 模型比较重要的超参数包括：神经网络
隐层节点个数、学习速率、词向量维数以及 RNN
的结构（普通/LSTM）。 

Intel 中国研究院曾采用基于 RNN 网络的学
习方法对韵律短语进行了预测,但并没有考虑语
义特征[7]。 

本文特别考虑词向量（word2vec）的信息作
为特征。word2vec 是 Google 在 2013 年年中开源
的一款将词表征为实数值向量的高效工具。一般
认为其是一个深度学习的模型，word2vec 通过训
练，可以把每个词表示为 N 维向量的一个特征。
从而两个词之间的距离可以用词向量空间的相似
度来表示。 

表 3    RNN 深度学习（普通结构）模型的 F-score 结果 

学习 

速率 

0.001 

0.0001 

0.001 

0.001 

0.001 

--- 

--- 

隐层节 

词向量 

语料库 1-

语料库 1- 

点个数 

维数 

韵律短语 

语调短语 

20 

20 

100 

100 

100 

--- 

--- 

0 

0 

30 

50 

100 

---- 

30 

78.07% 

68.52% 

70.19% 

64.45% 

79.56% 

71.97% 

79.01% 

72.14% 

75.39% 

53.49% 

57.25% 

61.34% 

61.34% 
35.75% 

注：最后两行分别为 LSTM 以及加入词向量后 CART 的对比结果 

实验表明，RNN 模型在引入词向量作为特征
之后，F-score 值由 78.07%提升至 79.56%。这说
明词向量特征可以较好的适应 RNN 模型。但在
CART 模型中加入词向量，反而会大大降低决策
树模型的准确性。 

3 模型的融合及多样性模型 

3.1 模型的线性融合 
        RNN 模型较其他模型相比，准确率有所提
升。但是其对于极少部分输入，会出现输出近似
1 或者 0 这样不够平滑的结果。通常的解决方法
是将其与其他模型作一个线性融合。 

本文选取 CART、RNN、CRF 模型所输出的

经过计算，CRF、CART、RNN 的融合系数
分别为 0.43、0.11、0.49，偏置项系数为-0.14。
可见结果对 RNN 和 CRF 的依赖程度远高于
CART。 

不同系数结果如下所示： 

表 5  线性融合模型的 F-score 结果 

RNN 

系数 

0.33 

0.43 

1 

0 

CRF 

系数 

0.33 

0.11 

0 

1 

CART 

系数 

偏执项 

语料库 1- 

系数 

韵律短语 

0.33 

0.49 

0 

0 

180 

-0.14 

0 

0 

77.91% 

78.12% 

78.07% 

77.70% 

3.2 CART 融合模型 

为了提高进一步模型融合的准确率，将短语
长度的约束引入，实验中还加入了到上一个边界
距离、当前语法词音节数、下一个语法词音节数
这个三个特征，用 CART 对 RNN、CART、CRF
进行融合。具体训练步骤如下所示： 
Step1:根据多模型的融合结果，得到每个样本的
“到上一个边界的距离”。 
Step2:随机抽取 75%样本，用 CART 进行训练 
Step3:用剩余的 25%样本进行交叉验证，判断结
果是否达到预值。如果达到预值，结束训练。 
Step4:如果没有达到预值，用 CART 训练的结果，
更新短语长度约束特征，并转到 STEP2 继续训练 
这里需要考虑的超参数，除了 CART 自带的
超参数，还有判断结果是否理想的 F-score 值。即
如果结果优于此值，即停止训练。 

这里用多样性语料库 3 进行验证。对于每一
个边界，如果 8 个人中有 2/3/4 人停顿方式与结
果相同，则认为该停顿“正确”。如果每句话的每
一个边界均正确或者仅有一个边界错误，则认为
该句话韵律表达正确。 

表 6 CART 融合模型的多样性结果（句子正确率） 

投票人数 

语料库 3-韵律短语 

2 

3 

4 

854/1000 

824/1000 

718/1000 

概率结果以及部分重要特征进行模型的融合。 

3.3 多样性模型 

在不再考虑特征的情况下，首先用线性模型
来训练线性模型融合的系数。训练时选取如下的
指数损失函数： 

                          (1) 

其中

； 为分类器的输出值。 

无论是根据直观经验或者通过统计分析，都
容易得到这样一个结论，即不同人对韵律层次的
划分有着不同的习惯。下面举两个语料库 3 中的
例子以说明这一点。 
[例句 1-1]郁达夫是个|极其崇尚情感|注重自我表
现  的|浪漫作家 

()(,)yfxLyfe