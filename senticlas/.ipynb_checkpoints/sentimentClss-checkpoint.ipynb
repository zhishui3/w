{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded succeed\n",
      "0.7376311844077961\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "import thulac\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "import nltk\n",
    "import random\n",
    "from sklearn import feature_extraction  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "\n",
    "\n",
    "# 读入情感词典\n",
    "\n",
    "f= codecs.open(r'BosonNLP_sentiment_score.txt',mode='r',encoding='utf-8')\n",
    "text=f.readlines()\n",
    "f.close()\n",
    "senDict ={}\n",
    "for s in text:\n",
    "    s=re.sub(r'[\\r\\t\\n\\u3000]','',s)\n",
    "    s=s.split(' ')\n",
    "    if len(s)==2:\n",
    "        senDict[s[0]] = np.float_(s[1])\n",
    "\n",
    "# 否定词\n",
    "notlist='不、没、无、非、莫、弗、勿、毋、未、否、别、无、休、难道'\n",
    "notDict=notlist.split('、')\n",
    "\n",
    "#程度词\n",
    "f= codecs.open(r'degreeAdv.txt',mode='r',encoding='utf-8')\n",
    "text=f.readlines()\n",
    "f.close()\n",
    "degreeDict ={}\n",
    "for s in text:\n",
    "    s=re.sub(r'[\\r\\n\\u3000\\ufeff]','',s)\n",
    "    s=re.split('\\t',s)\n",
    "    if len(s)==2:\n",
    "        degreeDict[s[0]] = np.float_(s[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f=open(r'stopwords.txt','r',encoding='utf-8')\n",
    "raw=f.read()\n",
    "f.close()\n",
    "punc=['(',')','，','。','？','“','”','‘','’','：','；','【','】','…','《','■','》','_','<','>','！',',',  '!', '.',  '、', '~', '）', '（', ';', ':']\n",
    "sent_mark=['，','。','？','！',',',  '!', '.']\n",
    "stopwords=raw.split()\n",
    "\n",
    "\n",
    "# 读入文本\n",
    "\n",
    "def score_sent(sent):\n",
    "    not_num=0\n",
    "    degree=1\n",
    "    score=0\n",
    "    for w in sent:\n",
    "        if w in notDict:\n",
    "            not_num+=1\n",
    "        if w in degreeDict:\n",
    "            degree=degree*degreeDict[w]\n",
    "        if w in senDict:\n",
    "            score=score+senDict[w]\n",
    "    score=(-1)**not_num*degree*score\n",
    "    return score\n",
    "\n",
    "f=open(r'review_sentiment\\train2.rlabelclass','r')\n",
    "raw=f.read()\n",
    "f.close()\n",
    "file_names=raw.split('\\n')\n",
    "\n",
    "file_names=[(f.split(' ')[0], f.split(' ')[1] )for f in file_names if len(f)>0]\n",
    "random.shuffle(file_names)\n",
    "file_sent=[s for (f,s) in file_names] #文本的情感标记\n",
    "files=[f for (f,s) in file_names]   #文件名\n",
    "\n",
    "thu1= thulac.thulac()\n",
    "dirname=r\"review_sentiment\\train2\\\\\"\n",
    "sent_doc=[] #句子的情感分值\n",
    "target=[]   # 句子的人工情感标记\n",
    "fn=0\n",
    "fm=0 #正确读入文件数量\n",
    "\n",
    "for fn in range(len(files)):\n",
    "    try:\n",
    "        f= open(dirname+files[fn],mode='r')#,encoding=\"utf-8\")\n",
    "        raw=f.read()\n",
    "        f.close()\n",
    "        raw=re.sub(r'[\\r\\n\\u3000]','',raw)\n",
    "        sents=re.split('[，。？！,!.\\t]',raw) #文本切分为句子，依据结束标记和符号\n",
    "        sents=[s for s in sents if len(s.strip())>0]\n",
    "\n",
    "        sents_score=[]\n",
    "        \n",
    "        for s in sents:\n",
    "            words=thu1.cut(s)\n",
    "            words=[w[0] for w in words]\n",
    "            words=[w for w in words if (w in senDict or w in notDict or w in degreeDict ) and w not in stopwords ]\n",
    "            sents_score.append(score_sent(words)) #计算每个句子的情感分值\n",
    "\n",
    "        #sents_score[-1]=sents_score[-1]*len(sents_score)/2\n",
    "        sent_doc.append(sum(sents_score))   #每个句子的情感分值的和，作为文本的情感分值\n",
    "        #sent_doc.append(sents_score[-1])\n",
    "        \n",
    "        target.append(file_sent[fn])\n",
    "        fm+=1\n",
    "        if fm>2000:\n",
    "            break\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "sent_doc=np.sign(np.array(sent_doc))\n",
    "\n",
    "target=[ int(t) for t in target]\n",
    "print(sum(sent_doc==target)/len(target))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......... linear ...........\n",
      "0.885\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.88      0.89      0.88       196\n",
      "        pos       0.89      0.88      0.89       204\n",
      "\n",
      "avg / total       0.89      0.89      0.89       400\n",
      "\n",
      "--------------------\n",
      "准确率: 0.89\n",
      "......... rbf ...........\n",
      "0.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.00      0.00      0.00       196\n",
      "        pos       0.51      1.00      0.68       204\n",
      "\n",
      "avg / total       0.26      0.51      0.34       400\n",
      "\n",
      "--------------------\n",
      "准确率: 0.49\n",
      "......... poly ...........\n",
      "0.49\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.00      0.00      0.00       196\n",
      "        pos       0.51      1.00      0.68       204\n",
      "\n",
      "avg / total       0.26      0.51      0.34       400\n",
      "\n",
      "--------------------\n",
      "准确率: 0.49\n",
      "......... sigmoid ...........\n",
      "0.49\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.00      0.00      0.00       196\n",
      "        pos       0.51      1.00      0.68       204\n",
      "\n",
      "avg / total       0.26      0.51      0.34       400\n",
      "\n",
      "--------------------\n",
      "准确率: 0.49\n"
     ]
    }
   ],
   "source": [
    " \n",
    "from matplotlib import pyplot  \n",
    "import scipy as sp  \n",
    "import numpy as np  \n",
    "from sklearn.cross_validation import train_test_split  \n",
    "from sklearn.feature_extraction.text import  CountVectorizer  \n",
    "from sklearn.feature_extraction.text import  TfidfVectorizer   \n",
    "from sklearn.metrics import precision_recall_curve  \n",
    "from sklearn.metrics import classification_report  \n",
    "from numpy import * \n",
    " \n",
    "\n",
    "#========SVM========#\n",
    "def SvmClass(x_train, y_train,st):\n",
    "\tfrom sklearn.svm import SVC \n",
    "\t#调分类器  \n",
    "\tclf = SVC(kernel = st,probability=True)#default with 'rbf' \n",
    "\tclf.fit(x_train, y_train)#训练，对于监督模型来说是 fit(X, y)，对于非监督模型是 fit(X)\n",
    "\treturn clf\n",
    "\n",
    "#========准确率召回率 ========#\n",
    "def Precision(clf):\n",
    "\tdoc_class_predicted = clf.predict(x_test) \n",
    "\tprint(np.mean(doc_class_predicted == y_test))#预测结果和真实标签\n",
    "\t#准确率与召回率  \n",
    "\tprecision, recall, thresholds = precision_recall_curve(y_test, clf.predict(x_test))  \n",
    "\tanswer = clf.predict_proba(x_test)[:,1]  \n",
    "\treport = answer > 0.5  \n",
    "\tprint(classification_report(y_test, report, target_names = ['neg', 'pos']))\n",
    "\tprint(\"--------------------\")\n",
    "\tfrom sklearn.metrics import accuracy_score\n",
    "\tprint('准确率: %.2f' % accuracy_score(y_test, doc_class_predicted))\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    data=[]\n",
    "    labels=[]\n",
    "    with open (\"train2.txt\",\"r\",encoding='utf-8')as file:\n",
    "        for line in file:\n",
    "            line=line[0:1]\n",
    "            labels.append(line)\n",
    "    with open(\"train2.txt\",\"r\",encoding='utf-8')as file:\n",
    "        for line in file:\n",
    "            line=line[1:]\n",
    "            data.append(line)\n",
    "    x=np.array(data)\n",
    "    labels=np.array(labels)\n",
    "    labels=[int (i)for i in labels]\n",
    "    movie_target=labels\n",
    "#转换成空间向量\n",
    "    count_vec = TfidfVectorizer(binary = False)\n",
    "#加载数据集，切分数据集80%训练，20%测试  \n",
    "    x_train, x_test, y_train, y_test= train_test_split(x, movie_target, test_size = 0.2)  \n",
    "    x_train = count_vec.fit_transform(x_train)  \n",
    "    x_test  = count_vec.transform(x_test)\n",
    "# \tprint('**************支持向量机************  ')\n",
    "    sts = ['linear','rbf','poly','sigmoid']\n",
    "    for st in sts:\n",
    "        print(\".........\",st,'...........')\n",
    "        Precision(SvmClass(x_train, y_train,st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.309 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "他 来自 北京 清华大学\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "c = \"他来自北京清华大学\"\n",
    "cc = jieba.cut(c)\n",
    "print(' '.join(cc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
