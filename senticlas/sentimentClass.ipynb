{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:\\\\pywork\\\\10.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-85ccf5688c9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m#遍历单个文件，读取行数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwritelines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m#f.write('\\n')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:\\\\pywork\\\\10.txt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "n = os.listdir(r'E:\\pywork\\nlp\\Sogou\\Sogou\\C000008')\n",
    "filenames = []\n",
    "for w in n:\n",
    "    filenames.append(os.path.abspath(w))\n",
    "#打开当前目录下的result.txt文件，如果没有则创建\n",
    "f=open('result.txt','w')\n",
    "#先遍历文件名\n",
    "for filename in filenames:\n",
    "    filepath = filename\n",
    "    #遍历单个文件，读取行数\n",
    "    for line in open(filepath):\n",
    "        f.writelines(line.strip())\n",
    "        #f.write('\\n')\n",
    "#关闭文件\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************支持向量机************  \n",
      "0.885\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.89      0.90      0.89       208\n",
      "        pos       0.89      0.88      0.88       192\n",
      "\n",
      "avg / total       0.89      0.89      0.89       400\n",
      "\n",
      "--------------------\n",
      "准确率: 0.89\n",
      "**************朴素贝叶斯************  \n",
      "0.8375\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.85      0.83      0.84       208\n",
      "        pos       0.82      0.84      0.83       192\n",
      "\n",
      "avg / total       0.84      0.84      0.84       400\n",
      "\n",
      "--------------------\n",
      "准确率: 0.84\n",
      "**************最近邻ＫＮＮ************  \n",
      "0.7025\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.79      0.58      0.67       208\n",
      "        pos       0.65      0.83      0.73       192\n",
      "\n",
      "avg / total       0.72      0.70      0.70       400\n",
      "\n",
      "--------------------\n",
      "准确率: 0.70\n",
      "**************逻辑回归************  \n",
      "0.8675\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.86      0.89      0.88       208\n",
      "        pos       0.88      0.84      0.86       192\n",
      "\n",
      "avg / total       0.87      0.87      0.87       400\n",
      "\n",
      "--------------------\n",
      "准确率: 0.87\n",
      "**************决策树************  \n",
      "0.8475\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.87      0.84      0.85       208\n",
      "        pos       0.83      0.86      0.84       192\n",
      "\n",
      "avg / total       0.85      0.85      0.85       400\n",
      "\n",
      "--------------------\n",
      "准确率: 0.85\n",
      "**************逻辑森林************  \n",
      "0.845\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.83      0.88      0.86       208\n",
      "        pos       0.86      0.81      0.83       192\n",
      "\n",
      "avg / total       0.85      0.84      0.84       400\n",
      "\n",
      "--------------------\n",
      "准确率: 0.84\n"
     ]
    }
   ],
   "source": [
    "#coding:utf-8\n",
    " \n",
    "from matplotlib import pyplot  \n",
    "import scipy as sp  \n",
    "import numpy as np  \n",
    "from sklearn.cross_validation import train_test_split  \n",
    "from sklearn.feature_extraction.text import  CountVectorizer  \n",
    "from sklearn.feature_extraction.text import  TfidfVectorizer   \n",
    "from sklearn.metrics import precision_recall_curve  \n",
    "from sklearn.metrics import classification_report  \n",
    "from numpy import * \n",
    " \n",
    " \n",
    " \n",
    "#========SVM========#\n",
    "def SvmClass(x_train, y_train):\n",
    "\tfrom sklearn.svm import SVC \n",
    "\t#调分类器  \n",
    "\tclf = SVC(kernel = 'linear',probability=True)#default with 'rbf' \n",
    "\tclf.fit(x_train, y_train)#训练，对于监督模型来说是 fit(X, y)，对于非监督模型是 fit(X)\n",
    "\treturn clf\n",
    " \n",
    "#=====NB=========#\n",
    "def NbClass(x_train, y_train):\n",
    "\tfrom sklearn.naive_bayes import MultinomialNB\n",
    "\tclf=MultinomialNB(alpha=0.01).fit(x_train, y_train) \n",
    "\treturn clf\n",
    " \n",
    "#========Logistic Regression========#\n",
    "def LogisticClass(x_train, y_train):\n",
    "\tfrom sklearn.linear_model import LogisticRegression\n",
    "\tclf = LogisticRegression(penalty='l2')\n",
    "\tclf.fit(x_train, y_train)\n",
    "\treturn clf\n",
    "\t\n",
    "#========KNN========#\n",
    "def KnnClass(x_train,y_train):\n",
    "\tfrom sklearn.neighbors import KNeighborsClassifier\n",
    "\tclf=KNeighborsClassifier()\n",
    "\tclf.fit(x_train,y_train)\n",
    "\treturn clf\n",
    " \n",
    " \n",
    "#========Decision Tree ========#\n",
    "def DccisionClass(x_train,y_train):\n",
    "\tfrom sklearn import tree\n",
    "\tclf=tree.DecisionTreeClassifier()\n",
    "\tclf.fit(x_train,y_train)\n",
    "\treturn clf\n",
    "\t\n",
    " \n",
    "#========Random Forest Classifier ========#\n",
    "def random_forest_class(x_train,y_train):\n",
    "\tfrom sklearn.ensemble import RandomForestClassifier\n",
    "\tclf= RandomForestClassifier(n_estimators=8)#参数n_estimators设置弱分类器的数量\n",
    "\tclf.fit(x_train,y_train)\n",
    "\treturn clf\n",
    " \n",
    "#========准确率召回率 ========#\n",
    "def Precision(clf):\n",
    "\tdoc_class_predicted = clf.predict(x_test) \n",
    "\tprint(np.mean(doc_class_predicted == y_test))#预测结果和真实标签\n",
    "\t#准确率与召回率  \n",
    "\tprecision, recall, thresholds = precision_recall_curve(y_test, clf.predict(x_test))  \n",
    "\tanswer = clf.predict_proba(x_test)[:,1]  \n",
    "\treport = answer > 0.5  \n",
    "\tprint(classification_report(y_test, report, target_names = ['neg', 'pos']))\n",
    "\tprint(\"--------------------\")\n",
    "\tfrom sklearn.metrics import accuracy_score\n",
    "\tprint('准确率: %.2f' % accuracy_score(y_test, doc_class_predicted))\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "\tdata=[]\n",
    "\tlabels=[]\n",
    "\twith open (\"train2.txt\",\"r\",encoding='utf-8')as file:\n",
    "\t\tfor line in file:\n",
    "\t\t\tline=line[0:1]\n",
    "\t\t\tlabels.append(line)\n",
    "\twith open(\"train2.txt\",\"r\",encoding='utf-8')as file:\n",
    "\t\tfor line in file:\n",
    "\t\t\tline=line[1:]\n",
    "\t\t\tdata.append(line)\n",
    "\tx=np.array(data)\n",
    "\tlabels=np.array(labels)\n",
    "\tlabels=[int (i)for i in labels]\n",
    "\tmovie_target=labels\n",
    "\t#转换成空间向量\n",
    "\tcount_vec = TfidfVectorizer(binary = False)\n",
    "\t#加载数据集，切分数据集80%训练，20%测试  \n",
    "\tx_train, x_test, y_train, y_test= train_test_split(x, movie_target, test_size = 0.2)  \n",
    "\tx_train = count_vec.fit_transform(x_train)  \n",
    "\tx_test  = count_vec.transform(x_test)\n",
    "\t\n",
    "\tprint('**************支持向量机************  ')\n",
    "\tPrecision(SvmClass(x_train, y_train))\n",
    "\tprint('**************朴素贝叶斯************  ')\n",
    "\tPrecision(NbClass(x_train, y_train))\n",
    "\tprint('**************最近邻ＫＮＮ************  ')\n",
    "\tPrecision(KnnClass(x_train,y_train))\n",
    "\tprint('**************逻辑回归************  ')\n",
    "\tPrecision(LogisticClass(x_train, y_train))\n",
    "\tprint('**************决策树************  ')\n",
    "\tPrecision(DccisionClass(x_train,y_train))\n",
    "\tprint('**************逻辑森林************  ')\n",
    "\tPrecision(random_forest_class(x_train,y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:0.0% Speed(reviews/sec):0 #Correct:0 #Trained:1 Training Accuracy:0.0%\n",
      "Progress:20.8% Speed(reviews/sec):9651. #Correct:1266 #Trained:2501 Training Accuracy:50.6%\n",
      "Progress:41.6% Speed(reviews/sec):5733. #Correct:2534 #Trained:5001 Training Accuracy:50.6%\n",
      "Progress:62.5% Speed(reviews/sec):5827. #Correct:3767 #Trained:7501 Training Accuracy:50.2%\n",
      "Progress:83.3% Speed(reviews/sec):5558. #Correct:5022 #Trained:10001 Training Accuracy:50.2%\n",
      "Progress:99.5% Speed(reviews/sec):5511. #Correct:5960 #Trained:11939 Training Accuracy:49.9%Progress:99.9% Speed(reviews/sec):6942. #Correct:2002 #Tested:4000 Testing Accuracy:50.0%.9%"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "f=open(r'E:\\pywork\\nlp\\work\\senticlas\\review_sentiment\\train2.rlabelclass','r')\n",
    "raw=f.read()\n",
    "f.close()\n",
    "file_names=raw.split('\\n')\n",
    "\n",
    "file_names=[(f.split(' ')[0], f.split(' ')[1] )for f in file_names if len(f)>0]\n",
    "random.shuffle(file_names)\n",
    "labels=[s for (f,s) in file_names] #文本的情感标记\n",
    "files=[f for (f,s) in file_names]   #文件名\n",
    "dirname=r\"E:\\pywork\\nlp\\work\\senticlas\\review_sentiment\\train2\\\\\"\n",
    "reviews = list()\n",
    "for fn in range(len(files)):\n",
    "    try:\n",
    "        f= open(dirname+files[fn],mode='r')#,encoding=\"utf-8\")\n",
    "        raw=f.read()\n",
    "        f.close()\n",
    "#         raw=re.sub(r'[\\r\\n\\u3000]','',raw)\n",
    "        reviews.append(raw)\n",
    "#         print(reviews)\n",
    "    except:\n",
    "        continue\n",
    "class SentimentNetwork:\n",
    "    def __init__(self, reviews,labels,min_count = 10,polarity_cutoff = 0.1,hidden_nodes = 10, learning_rate = 0.1):\n",
    "\n",
    "        np.random.seed(1)\n",
    "\n",
    "                    ################神经网络的数据预处理#################\n",
    "        self.pre_process_data(reviews, labels, polarity_cutoff, min_count)\n",
    "\n",
    "                    ##########神经网络的数据初始化###########\n",
    "        self.init_network(len(self.review_vocab),hidden_nodes, 1, learning_rate)\n",
    "\n",
    "\n",
    "                ###################################################\n",
    "                 #############神经网络的数据预处理函数实现#############\n",
    "                 ###################################################\n",
    "\n",
    "    def pre_process_data(self, reviews, labels, polarity_cutoff, min_count):\n",
    "\n",
    "    ###建立三个计数器分别对正面，负面，所有，进行计数\n",
    "        positive_counts = Counter()\n",
    "        negative_counts = Counter()\n",
    "        total_counts = Counter()\n",
    "\n",
    "        #对正面评论的单词进行计数\n",
    "        for i in range(len(reviews)):\n",
    "            if(labels[i] == '+1'):\n",
    "                for word in reviews[i].split(\" \"):\n",
    "                    positive_counts[word] += 1\n",
    "                    total_counts[word] += 1\n",
    "        #对负面评价的单词进行计数\n",
    "            else:\n",
    "                for word in reviews[i].split(\" \"):\n",
    "                    negative_counts[word] += 1\n",
    "                    total_counts[word] += 1\n",
    "\n",
    "        ###建立一个比率计数器\n",
    "        pos_neg_ratios = Counter()\n",
    "\n",
    "        ###对正面与反面的评论单词的比率进行计数\n",
    "        ###正面比率大则为正数，反面比率大则为负数\n",
    "        for term,cnt in list(total_counts.most_common()):\n",
    "            if(cnt >= 50):\n",
    "                pos_neg_ratio = positive_counts[term] / float(negative_counts[term]+1)\n",
    "                pos_neg_ratios[term] = pos_neg_ratio\n",
    "\n",
    "        for word,ratio in pos_neg_ratios.most_common():\n",
    "            if(ratio > 1):\n",
    "                pos_neg_ratios[word] = np.log(ratio)\n",
    "            else:\n",
    "                pos_neg_ratios[word] = -np.log((1 / (ratio + 0.01)))\n",
    "\n",
    "        ###只对出现总次数大于min_count以及比率介于±polarity_cutoff之间的单词进行统计\n",
    "        review_vocab = set()\n",
    "        for review in reviews:\n",
    "            for word in review.split(\" \"):\n",
    "                if(total_counts[word] > min_count):\n",
    "                    if(word in pos_neg_ratios.keys()):\n",
    "                        if((pos_neg_ratios[word] >= polarity_cutoff) or (pos_neg_ratios[word] <= -polarity_cutoff)):\n",
    "                            review_vocab.add(word)\n",
    "                    else:\n",
    "                        review_vocab.add(word)\n",
    "\n",
    "        #将词汇表转换为一个列表，这样我们就可以通过索引访问单词。\n",
    "        self.review_vocab = list(review_vocab)\n",
    "\n",
    "        # 对单词所对应的标签进行填充\n",
    "        label_vocab = set()\n",
    "        for label in labels:\n",
    "            label_vocab.add(label)\n",
    "\n",
    "        # 将标签词汇表转换为一个列表，这样我们就可以通过索引访问标签。\n",
    "        self.label_vocab = list(label_vocab)\n",
    "\n",
    "        #存储影评和标签词汇数组的大小。\n",
    "        self.review_vocab_size = len(self.review_vocab)\n",
    "        self.label_vocab_size = len(self.label_vocab)\n",
    "\n",
    "        #对索引的影评与标签重新编写字典\n",
    "        self.word2index = {}\n",
    "        for i, word in enumerate(self.review_vocab):\n",
    "            self.word2index[word] = i\n",
    "\n",
    "        self.label2index = {}\n",
    "        for i, label in enumerate(self.label_vocab):\n",
    "            self.label2index[label] = i\n",
    "\n",
    "            ###################################################\n",
    "            ##########神经网络的数据初始化的函数实现###############\n",
    "            ###################################################\n",
    "\n",
    "    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        #输入层、隐藏层、输出层的节点数量\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        #学习速率\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        #权重初始化\n",
    "        self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes))\n",
    "        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, \n",
    "                                                (self.hidden_nodes, self.output_nodes))\n",
    "\n",
    "        #初始化隐藏层数据\n",
    "        self.layer_1 = np.zeros((1,hidden_nodes))\n",
    "\n",
    "\n",
    "    #############标签数字化##############\n",
    "    def get_target_for_label(self,label):\n",
    "        if(label == '+1'):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    ###############激活函数：sigmoid函数################\n",
    "    def sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "     ###############sigmoid函数的倒数################\n",
    "    def sigmoid_output_2_derivative(self,output):\n",
    "        return output * (1 - output)\n",
    "\n",
    "\n",
    "    ###########################################################\n",
    "    ###################训练函数代码实现########################\n",
    "    ###########################################################\n",
    "\n",
    "    def train(self, training_reviews_raw, training_labels):\n",
    "\n",
    "    #标记条影评中每个出现的单词，对应在字典中记录下来作为输入层\n",
    "        training_reviews = list()\n",
    "        for review in training_reviews_raw:\n",
    "            indices = set()\n",
    "            for word in review.split(\" \"):\n",
    "                if(word in self.word2index.keys()):\n",
    "                    indices.add(self.word2index[word])\n",
    "            training_reviews.append(list(indices))\n",
    "\n",
    "        # 确保每个影评都有且仅有一个标签与其对应\n",
    "#         assert(len(training_reviews) == len(training_labels))\n",
    "\n",
    "       #记录预测正确的数量\n",
    "        correct_so_far = 0\n",
    "\n",
    "        # 记录时间\n",
    "        start = time.time()\n",
    "\n",
    "        #对每条影评学习的循环\n",
    "        for i in range(len(training_reviews)):\n",
    "\n",
    "            review = training_reviews[i]\n",
    "            label = training_labels[i]\n",
    "\n",
    "            #### 实现前向传播 ####\n",
    "\n",
    "            # 隐藏层的计算\n",
    "            self.layer_1 *= 0\n",
    "            for index in review:\n",
    "                self.layer_1 += self.weights_0_1[index]\n",
    "\n",
    "            # 输出层的计算\n",
    "            layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))            \n",
    "\n",
    "            ### 反向传播的实现 ###\n",
    "\n",
    "            # 输出误差计算\n",
    "            layer_2_error = layer_2 - self.get_target_for_label(label) \n",
    "            layer_2_delta = layer_2_error * self.sigmoid_output_2_derivative(layer_2)\n",
    "\n",
    "            # 反向传播误差计算\n",
    "            layer_1_error = layer_2_delta.dot(self.weights_1_2.T) \n",
    "            layer_1_delta = layer_1_error\n",
    "\n",
    "            # 更新权重\n",
    "            self.weights_1_2 -= self.layer_1.T.dot(layer_2_delta) * self.learning_rate \n",
    "\n",
    "            for index in review:\n",
    "                self.weights_0_1[index] -= layer_1_delta[0] * self.learning_rate # update input-to-hidden weights with gradient descent step\n",
    "\n",
    "            # 对预测情况进行判断\n",
    "            if(layer_2 >= 0.5 and label == '+1'):\n",
    "                correct_so_far += 1\n",
    "            elif(layer_2 < 0.5 and label == '-1'):\n",
    "                correct_so_far += 1\n",
    "\n",
    "            # 对预测以及学习情况进行即时输出\n",
    "            elapsed_time = float(time.time() - start)\n",
    "            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0\n",
    "\n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(training_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                             + \" #Correct:\" + str(correct_so_far) + \" #Trained:\" + str(i+1) \\\n",
    "                             + \" Training Accuracy:\" + str(correct_so_far * 100 / float(i+1))[:4] + \"%\")\n",
    "            if(i % 2500 == 0):\n",
    "                print(\"\")\n",
    "    ###################################################\n",
    "    ###################测试函数的实现##################\n",
    "    ###################################################\n",
    "    def test(self, testing_reviews, testing_labels):\n",
    "\n",
    "    #用于直接测试的函数，没有train函数的权重更新\n",
    "        correct = 0\n",
    "        start = time.time()\n",
    "        for i in range(len(testing_reviews)):\n",
    "            pred = self.run(testing_reviews[i])\n",
    "            if(pred == testing_labels[i]):\n",
    "                correct += 1\n",
    "\n",
    "            elapsed_time = float(time.time() - start)\n",
    "            reviews_per_second = i / elapsed_time if elapsed_time > 0 else 0\n",
    "\n",
    "            sys.stdout.write(\"\\rProgress:\" + str(100 * i/float(len(testing_reviews)))[:4] \\\n",
    "                             + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5] \\\n",
    "                             + \" #Correct:\" + str(correct) + \" #Tested:\" + str(i+1) \\\n",
    "                             + \" Testing Accuracy:\" + str(correct * 100 / float(i+1))[:4] + \"%\")\n",
    "    #################################################\n",
    "    ####################run函数的实现################\n",
    "    #################################################\n",
    "    def run(self, review):\n",
    "\n",
    "    #该函数通过数据的前向传播直接输出预测结果\n",
    "\n",
    "        self.layer_1 *= 0\n",
    "        unique_indices = set()\n",
    "        for word in review.lower().split(\" \"):\n",
    "            if word in self.word2index.keys():\n",
    "                unique_indices.add(self.word2index[word])\n",
    "        for index in unique_indices:\n",
    "            self.layer_1 += self.weights_0_1[index]\n",
    "        layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))\n",
    "        if(layer_2[0] >= 0.5):\n",
    "            return \"+1\"\n",
    "        else:\n",
    "            return \"-1\"\n",
    "mlp = SentimentNetwork(reviews,labels,min_count=20,polarity_cutoff=0.8,learning_rate=0.01)\n",
    "mlp.train(reviews,labels)\n",
    "f=open(r'E:\\pywork\\nlp\\work\\senticlas\\review_sentiment\\test2.rlabelclass','r')\n",
    "raw=f.read()\n",
    "f.close()\n",
    "file_names=raw.split('\\n')\n",
    "\n",
    "file_names=[(f.split(' ')[0], f.split(' ')[1] )for f in file_names if len(f)>0]\n",
    "random.shuffle(file_names)\n",
    "labls=[s for (f,s) in file_names] #文本的情感标记\n",
    "files=[f for (f,s) in file_names]   #文件名\n",
    "dirname=r\"E:\\pywork\\nlp\\work\\senticlas\\review_sentiment\\test2\\\\\"\n",
    "re = list()\n",
    "for fn in range(len(files)):\n",
    "    try:\n",
    "        f= open(dirname+files[fn],mode='r')#,encoding=\"utf-8\")\n",
    "        raw=f.read()\n",
    "        f.close()\n",
    "#         raw=re.sub(r'[\\r\\n\\u3000]','',raw)\n",
    "        re.append(raw)\n",
    "#         print(reviews)\n",
    "    except:\n",
    "        continue\n",
    "mlp.test(re,labls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
